# -*- mode: ruby -*-
# vi: set ft=ruby :
# Replace instances of 172.16.200.x to your local network

servers = [
    {
        :name => "k8s-master",
        :type => "master",
        :box => "centos/7",
        :box_version => "1905.1",
        :eth1 => "172.16.200.50",
        :mem => "4096",
        :cpu => "3"
    },
    {
        :name => "k8s-node-1",
        :type => "node",
        :box => "centos/7",
        :box_version => "1905.1",
        :eth1 => "172.16.200.51",
        :mem => "2048",
        :cpu => "2"
    },
    {
        :name => "k8s-node-2",
        :type => "node",
        :box => "centos/7",
        :box_version => "1905.1",
        :eth1 => "172.16.200.52",
        :mem => "2048",
        :cpu => "2"
    }
]

# This script to install k8s using kubeadm will get executed after a box is provisioned
$configureNetwork = <<-SCRIPT
    sed -i 's/ONBOOT=no/ONBOOT=yes/g' /etc/sysconfig/network-scripts/ifcfg-eth1
    sudo systemctl restart network
SCRIPT

$configureBox = <<-SCRIPT
    # install docker v18.09 - https://docs.docker.com/v17.09/engine/installation/linux/docker-ce/centos/#install-docker-ce-1
    VERSION=18.09.9

    # reason for not using docker provision in Vagrant config is that it always installs the latest version of Docker
    # kubeadm requires 18.09 or older
    sudo yum update -y
    sudo yum install -y yum-utils device-mapper-persistent-data lvm2
    sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
    
    # yum list docker-ce --showduplicates | sort -r
    # FUTURE IMPROVEMENT: add grep to get VERSION
    sudo yum install -y docker-ce-$VERSION docker-ce-cli-$VERSION containerd.io

    # exclude Docker from getting updated so it won't break compatibility
    sed -i '/distroverpkg=centos-release/a exclude=docker-ce*' /etc/yum.conf
    
    # run docker commands as vagrant user (sudo not required)
    usermod -aG docker vagrant

    ## Create /etc/docker directory.
    mkdir /etc/docker

    # Setup daemon.
    cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
EOF

    # Setup Docker service
    mkdir -p /etc/systemd/system/docker.service.d
    systemctl daemon-reload
    systemctl enable docker.service
    systemctl start docker.service

    # kubeadm prereqs
    # Ensure iptables tooling does not use the nftables backend
    sudo update-alternatives --set iptables /usr/sbin/iptables-legacy
    sudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy
    sudo update-alternatives --set arptables /usr/sbin/arptables-legacy
    sudo update-alternatives --set ebtables /usr/sbin/ebtables-legacy
    
    # Fix for incorrectly routed traffic
    cat <<EOF >  /usr/lib/sysctl.d/00-system.conf
# Enable netfilter on bridges.
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-arptables = 1
net.ipv4.ip_forward = 1
EOF
    sysctl --system

    # install kubeadm - https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
    cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
    
    # Set SELinux in permissive mode (effectively disabling it)
    setenforce 0
    sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
    
    # Install kube components
    yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
    systemctl enable --now kubelet

    # exclude Kube components from getting updated during normal maintenance
    sed -i 's/exclude=docker-ce\*/exclude=docker-ce* kubelet kubeadm kubectl/' /etc/yum.conf

    # kubelet requires swap off
    swapoff -a

    # keep swap off after reboot
    sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
SCRIPT

$configureMaster = <<-SCRIPT
    echo "This is a master node"
    # ip of this box
    IP_ADDR=`ip route get 1 | awk '{print $NF;exit}'`
    POD_CIDR="172.20.0.0/16"

    # firewall config to allow necessary master node ports
    sudo firewall-cmd --set-default-zone=home
    sudo firewall-cmd --zone=home --permanent --add-port=6443/tcp
    sudo firewall-cmd --zone=home --permanent --add-port=2379-2380/tcp
    sudo firewall-cmd --zone=home --permanent --add-port=10250-10252/tcp

    # install k8s master
    HOST_NAME=$(hostname -s)
    kubeadm init --apiserver-advertise-address=$IP_ADDR --apiserver-cert-extra-sans=$IP_ADDR  --node-name $HOST_NAME --pod-network-cidr=$POD_CIDR
    
    # copying credentials to regular user - vagrant
    sudo --user=vagrant mkdir -p /home/vagrant/.kube
    cp -i /etc/kubernetes/admin.conf /home/vagrant/.kube/config
    chown $(id -u vagrant):$(id -g vagrant) /home/vagrant/.kube/config

    # install Calico pod network addon
    export KUBECONFIG=/etc/kubernetes/admin.conf
    curl https://docs.projectcalico.org/v3.9/manifests/calico.yaml -O
    sed -i -e "s?192.168.0.0/16?$POD_CIDR?g" calico.yaml
    kubectl apply -f calico.yaml

    # Create join shell script with v=2 for more verbosity to help with troubleshooting
    kubeadm token create --print-join-command >> /etc/kubeadm_join_cmd.sh
    sudo sed -i '$ s/$/ --v=2/' /etc/kubeadm_join_cmd.sh
    chmod +x /etc/kubeadm_join_cmd.sh

    # required for setting up password less ssh between guest VMs
    sudo sed -i "/^[^#]*PasswordAuthentication[[:space:]]no/c\PasswordAuthentication yes" /etc/ssh/sshd_config
    sudo service sshd restart

SCRIPT

$configureNode = <<-SCRIPT
    echo "This is a worker node"

    # firewall config to allow necessary worker node ports
    sudo firewall-cmd --set-default-zone=home
    sudo firewall-cmd --zone=home --permanent --add-port=10250/tcp
    sudo firewall-cmd --zone=home --permanent --add-port=30000-32767/tcp

    # Get and run script from master node to join cluster
    MASTER_NODE_IP="172.16.200.50"
    yum install -y sshpass
    sshpass -p "vagrant" scp -o StrictHostKeyChecking=no vagrant@$MASTER_NODE_IP:/etc/kubeadm_join_cmd.sh .
    sh ./kubeadm_join_cmd.sh
SCRIPT

Vagrant.configure("2") do |config|

    servers.each do |opts|
        config.vm.define opts[:name] do |config|

            config.vm.box = opts[:box]
            config.vm.box_version = opts[:box_version]
            config.vm.hostname = opts[:name]
            config.vm.network :public_network, ip: opts[:eth1] # bridged network
            #config.vm.network "private_network", type: "dhcp"

            config.vm.provider "virtualbox" do |v|

                v.name = opts[:name]
            	v.customize ["modifyvm", :id, "--groups", "/KubeDevelopment"]
                v.customize ["modifyvm", :id, "--memory", opts[:mem]]
                v.customize ["modifyvm", :id, "--cpus", opts[:cpu]]

            end

            # we cannot use this because we can't install the docker version we want - https://github.com/hashicorp/vagrant/issues/4871
            #config.vm.provision "docker"

            config.vm.provision "shell", inline: $configureNetwork

            # set default route via bridged interface for the command below that's run on master node 
            # IP_ADDR=`ip route get 1 | awk '{print $NF;exit}'`
            config.vm.provision "shell",
            run: "always",
            inline: "sudo ip route add 0.0.0.0/0 via 172.16.200.1"

            config.vm.provision "shell", inline: $configureBox

            if opts[:type] == "master"
                config.vm.provision "shell", inline: $configureMaster
            else
                config.vm.provision "shell", inline: $configureNode
            end

        end

    end

end 